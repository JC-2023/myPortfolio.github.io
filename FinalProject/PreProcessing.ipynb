{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb7f80",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Data Gathering and Preprocessing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2fa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download and read in json files\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "file1_path = 'squad_dev_set.json'\n",
    "file2_path = 'squad_train_set.json'\n",
    "data1 = load_json(file1_path)\n",
    "data2 = load_json(file2_path)\n",
    "\n",
    "combined_data = data1['data'] + data2['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Questions and Answers to Dataframe\n",
    "preprocessed_data = []\n",
    "i = 0\n",
    "for entry in combined_data:\n",
    "    for paragraph in entry['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            preprocessed_data.append({\n",
    "                'topic_id': i,\n",
    "                'cleaned_question': qa['question'],\n",
    "                'answer': qa['answers'][0]['text'] if qa['answers'] else None\n",
    "            })\n",
    "    i+=1\n",
    "\n",
    "preprocessed_data_final = [qanda for qanda in preprocessed_data if qanda['answer'] is not None]\n",
    "pd.DataFrame(preprocessed_data_final).to_csv(\"qAndA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify Relevant wikipedia pages\n",
    "topics = list(pd.DataFrame(combined_data)['title'])\n",
    "wikiTopics = []\n",
    "i = 0\n",
    "#(rakshitarora)\n",
    "for topic in topics:\n",
    "    searchTopic = re.sub(r'%A9', 'e', topic)\n",
    "    searchTopic = re.sub(r'%..', '', searchTopic).strip()\n",
    "    print(searchTopic, i)\n",
    "    if len(searchTopic) >= 1:\n",
    "        topico = wikipedia.search(searchTopic)[0]\n",
    "        wikiTopics.append(topico)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull Wikipedia page data\n",
    "#Chat Assistance\n",
    "#(rakshitarora)\n",
    "pages = []\n",
    "i = 0\n",
    "for topic in wikiTopics:\n",
    "    try:\n",
    "        print(topic)\n",
    "        pages.append(wikipedia.page(topic))\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"Disambiguation error for topic: {topic}. Options: {e.options}\")\n",
    "        pages.append(None)\n",
    "    except wikipedia.exceptions.PageError as e:\n",
    "        print(\"Page not found\")\n",
    "        pages.append(None)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract information from HTML\n",
    "#Chat\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Assume `html_content` is the HTML of a Wikipedia page\n",
    "def get_main_body_text(html_content):\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Locate the main content div\n",
    "    main_content = soup.find('div', class_='mw-parser-output')\n",
    "    \n",
    "    # Extract the text from paragraphs within the main content\n",
    "    if main_content:\n",
    "        paragraphs = main_content.find_all('p')\n",
    "        body_text = ' '.join([p.get_text(strip=False) for p in paragraphs])\n",
    "        return body_text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "context = []\n",
    "i = 0\n",
    "for page in pages:\n",
    "    if page != None:\n",
    "        text = get_main_body_text(page.html())\n",
    "        if text != None:\n",
    "            context.append(text)\n",
    "    else:\n",
    "        context.append(None)\n",
    "    i += 1\n",
    "    print(i)\n",
    "    \n",
    "context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean context\n",
    "\n",
    "contexto = context.copy()\n",
    "def clean_text(content):\n",
    "    content = re.sub(r'<.*?>', '', content)\n",
    "    content = re.sub(r'\\[\\d+\\]', '', content)\n",
    "    #content = re.sub(r'.', '', content)\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "    return content\n",
    "\n",
    "for i in range(len(contexto)):\n",
    "    if contexto[i] != None:\n",
    "        contexto[i] = clean_text(contexto[i])\n",
    "    else:\n",
    "        contexto[i] = ''\n",
    "#ChatGPT\n",
    "with open('context.txt', \"w\") as file:\n",
    "    for line in contexto:\n",
    "        file.write(line + \"\\n\")\n",
    "\n",
    "print(contexto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly select questions with available context\n",
    "#(Janiszewski)\n",
    "from random import random\n",
    "question_answer = []\n",
    "indices = len(preprocessed_data_final)\n",
    "while len(question_answer) < 25:\n",
    "    index = int(random() * indices)\n",
    "    qa = preprocessed_data_final[index]\n",
    "\n",
    "    if context[qa['topic_id']] != None:\n",
    "        question_answer.append(qa)\n",
    "\n",
    "pd.DataFrame(question_answer).to_csv(\"questions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
